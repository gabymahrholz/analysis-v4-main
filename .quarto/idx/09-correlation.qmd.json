{"title":"Correlations","markdown":{"headingText":"Correlations","headingAttr":{"id":"sec-cor","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{r include=FALSE}\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(webexercises)\n```\n\n\n## Intended Learning Outcomes {.unnumbered}\n\nBy the end of this chapter you should be able to:\n\n- Compute a Pearson correlation and effectively report the results.\n- Understand when to use a non-parametric equivalent of correlation, compute it, and report the results.\n\n\n## [Individual Walkthrough]{style=\"color: #F39C12; text-transform: uppercase;\"} {.unnumbered}\n\n## Activity 1: Setup & download the data\n\nThis week, we will be working with a new dataset. Follow the steps below to set up your project:\n\n* **Create a new project** and name it something meaningful (e.g., \"2A_chapter9\", or \"09_correlation\"). See @sec-project if you need some guidance.\n* **Create a new `.Rmd` file** and save it to your project folder. See @sec-rmd if you need help. \n* Delete everything after the setup code chunk (e.g., line 12 and below) \n* **Download the new dataset** here: [data_ch9.zip](data/data_ch9.zip \"download\"). The zip folder includes the following files:\n  * `Soupbowl_data_dictionary.xlsx`: A codebook with detailed variable descriptions\n  * `data_ch9_correlation.csv`: A CSV file containing data from the experiment and demographic information\n  * A copy of the paper by Lopez et al. (2024) and the Supplementary materials\n* Extract the data files from the zip folder and place them in your project folder. If you need help, see @sec-download_data_ch1.\n\n\n**Citation**\n\n> Lopez, A., Choi, A. K., Dellawar, N. C., Cullen, B. C., Avila Contreras, S., Rosenfeld, D. L., & Tomiyama, A. J. (2024). Visual cues and food intake: A preregistered replication of Wansink et al. (2005). *Journal of Experimental Psychology: General, 153*(2), 275–281.  [https://doi.org/10.1037/xge0001503](https://doi.org/10.1037/xge0001503){target=\"_blank\"}\n\nThe preregistration, data and supplementary materials are available on OSF & APA's \"Journal of Experimental Psychology: General\": \n\n* Pre-reg: [https://osf.io/ux42g](https://osf.io/ux42g){target=\"_blank\"}\n* Data: [https://osf.io/8q647/](https://osf.io/8q647/){target=\"_blank\"}\n* SupMats: [https://supp.apa.org/psycarticles/supplemental/xge0001503/xge0001503.pdf](https://supp.apa.org/psycarticles/supplemental/xge0001503/xge0001503.pdf){target=\"_blank\"}\n\n**Abstract**\n\n> Imagine a bowl of soup that never emptied, no matter how many spoonfuls you ate—when and how would you know to stop eating? Satiation can play a role in regulating eating behavior, but research suggests visual cues may be just as important. In a seminal study by [Wansink et al. (2005)](https://doi.org/10.1038/oby.2005.12){target=\"_blank\"}, researchers used self-refilling bowls to assess how visual cues of portion size would influence intake. The study found that participants who unknowingly ate from self-refilling bowls ate more soup than did participants eating from normal (not self-refilling) bowls. Despite consuming 73% more soup, however, participants in the self-refilling condition did not believe they had consumed more soup, nor did they perceive themselves as more satiated than did participants eating from normal bowls. Given recent concerns regarding the validity of research from the Wansink lab, we conducted a preregistered direct replication study of Wansink et al. (2005) with a more highly powered sample (N =464 vs. 54 in the original study). We found that most results replicated, albeit with half the effect size (d= 0.45 instead of 0.84), with participants in the self-refilling bowl condition eating significantly more soup than those in the control condition. Like the original study, participants in the selfrefilling condition did not believe they had consumed any more soup than participants in the control condition. These results suggest that eating can be strongly controlled by visual cues, which can even override satiation.\n\n**Public Significance Statement**\n\n> Results from this study are relevant to public health and science given the influence that the bottomless soup bowls study had on public policy and the skepticism surrounding research from the Wansink lab. We found that what the eyes see plays a significant role in how much people eat and how full they feel. Given the high prevalence of diseases of overconsumption, this study has implications for the regulation of eating behavior.\n\n**Changes made to the dataset**\n\n* The basis for `data_ch9_correlation.csv` is the file named `included&excludedFINAL.sav` on OSF. It contains 632 observations.\n* The original authors used an SPSS file format, which was exported as a csv.\n* The original authors coded missing values as 999 and 888. These were replaced with actual missing values (NA).\n* Variables `M_postsoup` and `Condition` were removed so we can practice more data wrangling. It also gives us an opportunity to include one of Gaby's favourite functions `coalesce()`.\n* No other changes were made.\n\n\n## Activity 2: Library and data for today\n\nToday, we will use the following packages: `tidyverse`, `ggExtra`, `correlation`, `qqplotr`, and `pwr`. As always, you may need to install any packages you have not used/ installed before (see @sec-install_packages if you need some help).\n\nAdditionally, we will read in today’s data from `data_ch9_correlation`.\n\n\n\n```{r eval=FALSE}\n# load in the packages\n???\n\n# read in the data\nlopez_data <- ???\n```\n\n\n```{r include=FALSE, message=TRUE}\n## I basically have to have 2 code chunks since I tell them to put the data files next to the project, and mine are in a separate folder called data - unless I'll turn this into a fixed path\nlibrary(tidyverse)\nlibrary(ggExtra)\nlibrary(correlation)\nlibrary(qqplotr)\nlibrary(pwr)\n\n# read in the data\nlopez_data <- read_csv(\"data/data_ch9_correlation.csv\")\n```\n\n\n\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Solution \n\n```{r eval=FALSE}\n# load in the packages\nlibrary(tidyverse)\nlibrary(ggExtra)\nlibrary(correlation)\nlibrary(qqplotr)\nlibrary(pwr)\n\n# read in the data\nlopez_data <- read_csv(\"data_ch9_correlation.csv\")\n```\n\n:::\n\n## Activity 3: Familiarise yourself with the data\n\nAs usual, take some time to familiarise yourself with the data before starting the analysis. Review the codebook to understand the variables and their measurement.\n\nWe notice that `lopez_data` contains data from 632 participants, whereas the Lopez et al. (2024) paper reports a total of 654 participants. Have a look at the data and the paper. Can you explain the discrepancy?\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Explanation\n\nLopez et al. appear to have published data for both the participants included in the final sample (464 participants) and those excluded after discovering the true purpose of the study (168 participants). However, an additional 22 participants were excluded due to experimenter error, and their data were not recorded.\n:::\n\nToday, we will focus on a few correlation values and attempt to reproduce some of the results presented in Table 2 of Lopez et al. (2024).\n\n\n![Table 2 (Lopez et al., 2024)](images/lopez_table2.png)\n\nAs shown in the table above, the original authors aimed to replicate the correlations reported by Wansink et al. (2005). Specifically, they examined the following relationships:\n\n* Estimated ounces of soup consumed with actual consumption volume\n* Estimated calories of soup consumed with actual consumption volume\n\nThese correlations were calculated for three groups: participants who ate from normal bowls, participants who ate from self-refilling bowls, and the combined group of all participants.\n\n\n## Activity 4: Preparing the dataframe\n\n\nTo calculate correlations, the data must be in wide format. Fortunately, `lopez_data` is already in wide format. However, if your data is organised differently, ensure that each variable is in its own column.\n\n\n#### Step 1: Excluding participants' data {.unnumbered}\n\nCheck that only participants who meet the recruitment criteria are included, and exclude those who do not. Lopez and colleagues made this straightforward by including the variable `Included` in the dataset. This variable indicates which participants were included in the final dataset (value of 1) and which were excluded (value of 0). After completing this step, our new data object should contain 464 participants.\n\n::: {.callout-note collapse=\"true\" icon=\"false\"} \n\n## Your Turn\n\nCreate a new data object named `lopez_included` that contains only the participants marked as included in the final dataset (i.e., those with a value of 1 in the `Included` variable).\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Solution \n\n```{r}\nlopez_included <- lopez_data %>% \n  filter(Included == 1)\n```\n:::\n\n:::\n\n#### Step 2: Selecting relevant variables {.unnumbered}\n\nBig datasets can be messy, so it is often helpful to focus only on the variables relevant to your analysis. For this task, we want to include the following variables:\n\n* `ParticipantID`\n* Estimated ounces of food consumed (`OzEstimate`)\n* Estimated calories of food consumed (`CalEstimate`)\n* Consumption volume - Experimental soup intake (`ExpSoupAte`)\n* Consumption volume - Control soup intake (`CtrlSoupAte`)\n* `SeatPosition` - so we can determine whether participants were sorted into in the Experimental or the Control group \n\nStore the new data object as `lopez_reduced`.\n\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Solution \n\n```{r}\nlopez_reduced <- lopez_included %>% \n  select(ParticipantID, Sex, OzEstimate, CalEstimate, CtrlSoupAte, ExpSoupAte, SeatPosition)\n```\n:::\n\n\n#### Step 3: Recoding the values {.unnumbered}\n\nAs we can see now, our `lopez_reduced` data object could use some tidying. \n\n1. `Sex` is a categorical variable, yet, it is currently displayed as numeric, which isn't ideal. Recode these values according to the `Soupbowl_data_dictionary`. You could either recode the values as we did in @sec-wrangling2_act3) or convert `Sex` into a factor if that's easier (see @sec-dataviz for guidance).\n2. `ParticipantID` should also be converted into a factor (see @sec-dataviz for guidance).\n3. Create a new variable called `Condition` to indicate whether participants are in the Control or Experimental group. Use the `Soupbowl_data_dictionary` o determine which `SeatPosition` corresponds to normal bowls (Control) and self-refilling bowls (Experimental). If you’re unsure which function to use, refer to @sec-wrangling2_act4.\n4. `CtrlSoupAte` and `ExpSoupAte` can remain separate if we want to focus on these two conditions individually, as we can remove missing values later. However, to calculate correlations across the whole sample, we need these \"SoupAte\" values in a single column instead of two. We will name this coalesced variable `M_postsoup` to align with the `Soupbowl_data_dictionary`.\n\nYou should be able to complete recoding tasks 1, 2, and 3 on your own. However, Task 4 involves a new function, so we will walk through it together.\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Solution for Recoding tasks 1-3\n\nIn the panels below, I have created new variables in `lopez_reduced` to show you solutions for both the `case_match()` and the `factor()` approach for recoding `Sex`. Alternatively, you could simply name your new column `Sex` to overwrite the numeric values in the existing `Sex` column.\n\n\n::: {.panel-tabset group=\"layers\"}\n\n## Sex with `case_match()`\n\nSince no conditional statement is needed, a simple `case_match()` will suffice. This will create a character column.\n\n```{r}\nlopez_reduced <- lopez_reduced %>% \n  mutate(Sex_case_match = case_match(Sex,\n                                     0 ~ \"Male\",\n                                     1 ~ \"Female\",\n                                     2 ~ \"Other\",\n                                     3 ~ \"Prefer not to say\"))\n```\n\n\n## Sex with `factor()`\n\nUsing a factor is helpful if you want to reorder the labels simultaneously. This will create a factor column.\n\n```{r warning=FALSE}\nlopez_reduced <- lopez_reduced %>% \n  mutate(Sex_factor = factor(Sex,\n                         levels = c(1, 0, 2, 3),\n                         labels = c(\"Female\", \"Male\", \"Other\", \"Prefer not to say\")))\n```\n\n## ParticipantID with `factor()`\n\nThere is not much to it, as the levels and labels don't need changing.\n\n```{r warning=FALSE}\nlopez_reduced <- lopez_reduced %>% \n  mutate(ParticipantID = factor(ParticipantID))\n```\n\n\n## Condition with `case_when()`\n\nAccording to the `Soupbowl_data_dictionary`, even numbers in `SeatPosition` indicate the Experimental group, and odd numbers represent the Control group. Here we will need to use `case_when()` because **conditional statements** (see @sec-wrangling2 Activity 4 for a refresher).\n\n```{r warning=FALSE}\nlopez_reduced <- lopez_reduced %>% \n  mutate(Condition = case_when(\n    SeatPosition %in% c(1,3) ~ \"Control\",\n    SeatPosition %in% c(2,4) ~ \"Experimental\"\n  ))\n```\n\n\n::: {.callout-note} \nIn the code, I specified 1 and 3 should be labelled \"Control\" and 2 and 4 \"Experimental\". I could have used with the `.default = ` argument for the even seat numbers, however, this requires being certain that all other values are either 2 or 4 (e.g., no invalid numbers like 5 or missing values). Since I did not check beforehand, any value that is not 1, 2, 3, or 4 will remain unlabelled, making it easier to identify and address.\n\n:::\n\n:::\n\n:::\n\n\nWe want to combine the values from `CtrlSoupAte` and `ExpSoupAte` into a single column named `M_postsoup`. *(No idea why the authors called it `M_postsoup` rather than `AllSoupAte`. However, since this was one of the columns deleted from the original dataset earlier, we’ll stick with the original variable name as the authors intended.)*\n\nRecoding task 4 is straightforward with a function like `coalesce()`. For each row, it checks the specified columns and finds the first non-missing value at each position. Essentially, R scans the columns row by row, looking for the first available value that isn’t missing and assigns it to the new column. For example:\n\n* Participant 1001 has no value in `CtrlSoupAte` but has a value in `ExpSoupAte`, so R skips the missing value and assigns 4.5 to the new column.\n* Participant 1002 has a value in CtrlSoupAte (3.3), directly assigns it to the new column, and so on.\n\n```{r}\nlopez_reduced <- lopez_reduced %>% \n  mutate(M_postsoup = coalesce(CtrlSoupAte, ExpSoupAte))\n```\n\n\nAs always, all these \"preparing the dataframe\" steps could have been combined into a single pipe.\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Solution for Activity 4 in a single pipe\n\n```{r}\nlopez_reduced <- lopez_data %>% \n  # Step 1\n  filter(Included == 1) %>% \n  #Step 2\n  select(ParticipantID, Age, Sex, OzEstimate, CalEstimate, CtrlSoupAte, ExpSoupAte, SeatPosition) %>% \n  # Step 3 (I decided to use the factor approach and override my `Sex` column and include it all into the same `mutate()` function)\n  mutate(Sex = factor(Sex,\n                      levels = c(1, 0, 2, 3),\n                      labels = c(\"Female\", \"Male\", \"Other\", \"Prefer not to say\")),\n         ParticipantID = factor(ParticipantID),\n         Condition = case_when(\n           SeatPosition %in% c(1,3) ~ \"Control\",\n           SeatPosition %in% c(2,4) ~ \"Experimental\"),\n         # Step 4\n         M_postsoup = coalesce(CtrlSoupAte, ExpSoupAte))\n```\n:::\n\n\n\n## Activity 5: Compute descriptives\n\n\nNow that the data is ready, we can focus on reproducing the correlations. For the remainder of this chapter, we will focus on **correlating the estimated calories of soup consumption with actual consumption volume** for the **control group**. Testing every correlation would require assessing assumptions for each relationship, making this chapter unnecessarily long. However, you are welcome to reproduce all other correlations in your own time if you wish.\n\n\nFor the **descriptives**, we will compute the number of participants (**n**), **means** and **standard deviations** for the `Control` group of our variables of interest (i.e., `CalEstimate` and `M_postsoup`).\n\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Hints \n\n* Filter the data to include only the Control group and focus on the two variables of interest (`CalEstimate` and `M_postsoup`), and keep the `ParticipantID`. You may want to store the data as a new object named `lopez_control`.\n* Watch out for missing values in some of the columns. We will address them later during the correlation analysis. For now, instruct R to ignore missing values when calculating descriptive statistics.\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Solution\n\n```{r}\n## data object lopez_control\nlopez_control <- lopez_reduced %>% \n  filter(Condition == \"Control\") %>% \n  select(ParticipantID, CalEstimate, M_postsoup)\n\n# descriptives for lopez_control\ndescriptives_control <- lopez_control %>% \n  summarise(n = n(),\n            mean_CalEstimate = mean(CalEstimate, na.rm = TRUE),\n            sd_CalEstimate = sd(CalEstimate, na.rm = TRUE),\n            mean_M_postsoup = mean(M_postsoup),\n            sd_M_postsoup = sd(M_postsoup)) %>% \n  ungroup()\n\ndescriptives_control\n```\n\n\n:::\n\n:::\n\n## Activity 6: Create an appropriate plot\n\nThe only option for a correlation is a `r mcq(c(x = \"stacked barchart\", x = \"histogram\", answer = \"scatterplot\", x = \"violin-boxplot\"))` because `r mcq(c(x = \"both variables are categorical\", answer = \"both variables are continuous\", x = \"one variable is continuous and the other categorical\", x = \"There is only one variable and it's continuous\"))`.\n\n::: {.callout-note collapse=\"true\" icon=\"false\"} \n\n## Your Turn\n\nCreate an appropriate plot to visualise the relationship between `CalEstimate` and `M_postsoup`.\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n\n## Solution for the plot\n\n```{r}\nggplot(lopez_control, aes(x = CalEstimate, y = M_postsoup)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n:::\n\n:::\n\n## Activity 7: Check assumptions\n\n\n#### Assumption 1: Continuous variables {.unnumbered}\n\nBoth variables must be measured at the **interval or ratio** level (i.e., continuous data). We can confirm this by examining our two variables of interest, `CalEstimate` and `M_postsoup`. Plus, you would already know whether the variables are continuous based on your study design.\n\n\n#### Assumption 2: Related pairs {.unnumbered}\n\nEach participant needs to have **2 data points**, in our case one value for `CalEstimate` and one for `M_postsoup`. You can verify this directly in the dataset (e.g., `lopez_control`) or watch for warning messages when creating the scatterplot.\n\nIn this dataset, there are no missing values in `M_postsoup` but `CalEstimate` has two missing values. hmmm. Interestingly, the Lopez paper mentions exclusion criteria in the Supplementary Materials but only in relation to participants, not statistical exclusions.\n\nTechnically, incomplete pairs won’t appear in the scatterplot and will be automatically excluded during the correlation analysis (as we will observe later). However, in the interest of transparency, any data exclusions should be clearly stated in your reports.\n\nTo address this, we can also exclude the missing values from `lopez_control`. Here, I'm using the `drop_na()` function to remove rows with missing data. I will save this as a new data object `lopez_control_no_na`. \n\n```{r}\nlopez_control_no_na <- lopez_control %>% \n  drop_na()\n```\n\n\nData from participants 1895 and 1908 has now been excluded, leaving us with complete pairs. The assumption holds.\n\n\n#### Assumption 3: Independence of cases/observations {.unnumbered}\n\nSimilar to the paired t-test, we must assume that the two **observations for each case are independent** of the observations for any other case. This basically means that participants did not influence each other's ratings. Once again, this assumption is related to the study design and is satisfied in this instance.\n\n\n#### Assumption 4: Linearity {.unnumbered}\n\nTo run a Pearson correlation, the relationship between the two variables must be linear.\n\nYou can assess linearity visually using a **Residuals vs Fitted plot**, which can be generated by applying the `plot()` function to a linear model (`lm`) object. The function requires the following arguments:\n\n* The 2 variables of interest, separated by a tilde (`~`)\n* The dataset\n* The `which` argument specifies which plot we want to show. Number 1 will produce a Residuals vs Fitted plot.\n\n```{r}\nplot(lm(CalEstimate~M_postsoup, data = lopez_control_no_na), which = 1)\n```\n\n::: {.callout-note collapse=\"true\" icon=\"false\"} \n\n## Alternative\n\nYou can also assess linearity using a scatterplot, but instead of fitting a linear model \"lm\", use the \"loess\" method for the line of best fit. Then, examine how closely the resulting line resembles a linear line.\n\n\n```{r}\nggplot(lopez_control_no_na, aes(x = CalEstimate, y = M_postsoup)) +\n  geom_point() +\n  geom_smooth(method = \"loess\")\n```\n\n:::\n\n**Verdict:** This does actually look non-linear to me. The original authors did not mention any assumption testing in their paper or Supplementary Materials. If they conducted these tests, we can assume they deemed the assumptions satisfied.\n\n\n#### Assumption 5: Normality {.unnumbered}\n\nResiduals for both variables should follow a **bivariate normal distribution** (i.e., both together normally distributed). We can use a **Q-Q plot** to visually check this assumptions. The code is the same as above (i.e., `plot()` with `lm()`), but this time, set the `which` argument to 2 to generate the Q-Q plot.\n\n```{r}\nplot(lm(CalEstimate~M_postsoup, data = lopez_control_no_na), which = 2)\n```\n\n::: {.callout-note collapse=\"true\" icon=\"false\"} \n\n## Alternatives\n\n\n::: {.panel-tabset group=\"layers\"}\n\n## Option 1: plotting both variables individually\n\nIn practice it is frequently accepted that each variable is normally distributed rather than both together (i.e., **univariate normality**).\nHowever, there is some disagreement among experts whether Pearson’s correlation is \"robust\" to violations of univariate normality. Anyway, we are showing this approach as an alternative using the Q-Q plot version with packages `ggplot` and `qqplotr`.\n\n```{r eval=FALSE, message = FALSE}\nggplot(lopez_control_no_na, aes(sample = CalEstimate)) +\n  stat_qq_band(fill = \"#FB8D61\", alpha = 0.4) +\n  stat_qq_line(colour = \"#FB8D61\") +\n  stat_qq_point()\n\nggplot(lopez_control_no_na, aes(sample = M_postsoup)) +\n  stat_qq_band(fill = \"#FB8D61\", alpha = 0.4) +\n  stat_qq_line(colour = \"#FB8D61\") +\n  stat_qq_point()\n```\n\n```{r fig-bins, fig.cap=\"Bins vs binwidth arguments\", echo=FALSE}\na <- ggplot(lopez_control_no_na, aes(sample = CalEstimate)) +\n  stat_qq_band(fill = \"#FB8D61\", alpha = 0.4) +\n  stat_qq_line(colour = \"#FB8D61\") +\n  stat_qq_point()\n\nb <- ggplot(lopez_control_no_na, aes(sample = M_postsoup)) +\n  stat_qq_band(fill = \"#FB8D61\", alpha = 0.4) +\n  stat_qq_line(colour = \"#FB8D61\") +\n  stat_qq_point()\n\n# add plots together in 1 row\na + b + plot_layout(nrow = 1)\n```\n\n## Option 2: Scatterplot \n\nWe can also use the function `ggMarginal()` to enhance a ggplot2 scatterplot by adding marginal density plots, histograms, or boxplots. The workflow is slightly different: first, save the scatterplot as a data object, then apply `ggMarginal()` to that object.\n\nTo customise the marginal plots:\n\n* Use `type = \"histogram\"` for histograms.\n* Use `type = \"boxplot\"` for boxplots. \n* Leave the `type` argument out or set it to `type = \"density\"` to display the default density plots.\n\n```{r}\np1 <- ggplot(lopez_control_no_na, aes(x = CalEstimate, y = M_postsoup)) +\n  geom_point()\n\nggMarginal(p1, type = \"density\")\n```\n\n\n\nMore info here: [https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html](https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html){target=\"_blank\"}\n:::\n\n:::\n\n**Verdict:** For me, the assumption of normality would not hold, whether assessed jointly or independently, as the points follow a curve rather than \"hugging\" the straight line with potentially some potential deviations in the tails. However, I seem to be more conservative in my judgements compared to Lopez et al. Since they proceeded with a Pearson correlation, the assumption must have held for them.\n\n\n\n#### Assumption 6: Homoscedasticity {.unnumbered}\n\nHomoscedasticity assumes that the data is evenly distributed around the line of best fit, with no visible pattern. This assumption can be assessed visually using a **Scale-Location plot** or directly within the scatterplot.\n\n\n::: {.panel-tabset group=\"layers\"}\n\n## Option 1: Scale-Location plot\n\nAs you might have guessed, we will use the `plot()` function with `lm()` again but this time set the `which` argument to 3 to generate the Scale-Location plot.\n\n```{r}\nplot(lm(CalEstimate~M_postsoup, data = lopez_control_no_na), which = 3)\n```\n\n\n## Option 2: Scatterplot\n\nWe will use the same scatterplot as above, but this time focus on the points and their distance from the blue line. If the points are randomly scattered around the line of best fit, the assumption holds. However, if a distinct pattern emerges, the assumption is violated.\n\n```{r}\nggplot(lopez_control_no_na, aes(x = CalEstimate, y = M_postsoup)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n\n:::\n\n**Verdict:** This is another assumption that feels borderline to me. There appears to be slight heteroscedasticity. In the Scale-Location plot, the red line should ideally be flat and horizontal, but ours has a slight slope and could be flatter. If you prefer looking at the scatterplot, you can observe the data points moving slightly further away from the line of best fit as the x and y values increase. This is kinda forming a slight funnel shape rather than the roughly random spread of data points shown in last week’s lecture slides. Nevertheless, the authors took a more lenient approach and considered the assumption to be met.\n\n\n#### Assumption 7: Absence of outliers {.unnumbered}\n\nIdeally, our data should not contain any outliers or influential points, as these can distort the relationship between the variables of interest. We can assess this assumption visually with a **Residuals vs Leverage plot** or by revisiting the scatterplot (as shown above).\n\n::: {.panel-tabset group=\"layers\"}\n\n## Option 1: Residuals vs Leverage plot\n\nTo identify those data points, we can look at plot 5 from the `plot()` function with the `lm()` object but setting the `which` argument to 5.\n\n```{r}\nplot(lm(CalEstimate~M_postsoup, data = lopez_control_no_na), which = 5)\n```\nHere, case numbers 3, 183, and 205 are identified. In the data object `lopez_control_no_na`, these correspond to participant IDs 1007, 1716, and 1789, respectively. Two of these participants estimated they had consumed 800 calories, while the third data point has a y-value over 40 (the one with the lower x-value).\n\n## Option 2: Scatterplot\n\nLooking at the scatterplot, I would have probably identified 4 outliers - anyone who estimated more than 600 cal and anyone who consumed more than 40 ounces of soup.\n\n:::\n\n**Verdict:** There are several ways to handle outliers, such as excluding them, replacing their extreme values with the group mean, or winsorising them. For now, I would likely exclude these three data points from the dataset before proceeding. However, Lopez et al. chose to retain all data points.\n\n**Remember: Any data exclusions in your reports must be clearly justified.**\n\n::: {.callout-note icon=\"false\"} \n\n## Your Turn\n\nFilter out the 3 (or 4) influential points/outliers and create the plot again. It’s best to save the data without outliers as a new data object. I’ve named mine `lopez_control_no_outliers`, but feel free to use a name that works for you.\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n## Solution for removing 3 outliers \n\n```{r}\nlopez_control_no_outliers <- lopez_control_no_na %>% \n  filter(!ParticipantID %in% c(1007, 1716, 1789))\n```\n\n\n```{r}\nggplot(lopez_control_no_outliers, aes(x = CalEstimate, y = M_postsoup)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n\n:::\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"} \n## Solution for removing 4 outliers \n\n```{r}\nlopez_control_no_outliers4 <- lopez_control_no_na %>% \n  filter(CalEstimate < 600,\n         M_postsoup < 40)\n```\n\n\n```{r}\nggplot(lopez_control_no_outliers4, aes(x = CalEstimate, y = M_postsoup)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n\n:::\n\n:::\n\n\n#### Final word on Assumption checks {.unnumbered}\n\nFor me, there would be enough justification to switch to a non-parametric correlation method, such as Spearman. However, since the original authors ran Pearson correlations, we will follow their approach in Activity 8. We will also revert to the data object `lopez_control`, as the original authors chose not to exclude any data points (e.g., missing values, outliers, etc.).\n\n\n\n## Activity 8: Pearson's correlation & effect size\n\nThere are plenty of options out there to compute a correlation. We will use the `correlation()` function from the `correlation` package because it offers more consistent reporting features. The `correlation()` function requires:\n\n* The name of the data set you are using.\n* The name of the first variable you want to select for the correlation.\n* The name of the second variable you want to select for the correlation.\n* The type of correlation you want to run: e.g. Pearson, Spearman.\n\nFor a two-tailed Pearson correlation using the `lopez_control` dataset, the code would look like this *(note the quotation marks around everything except the data object)*:\n\n```{r}\ncorrelation(data = lopez_control,\n            select = \"CalEstimate\",\n            select2 = \"M_postsoup\",\n            method = \"Pearson\",\n            alternative = \"two.sided\")\n```\n\n::: {.callout-tip} \n\nSince `lopez_control` contains only three variables and we’ve already converted `ParticipantID` into a factor, we can pass the entire data object to the `correlation()` function. This will compute correlations between all numeric columns in the dataset. In this case, it would result in just one association, but see below for an example with more numeric variables.\n\n```{r}\ncorrelation(lopez_control)\n```\n\n:::\n\nHere we obtained a Pearson correlation value of .405 which is close to the value of .41 reported by Lopez et al. (2024). Remember to report correlation values to three decimal places to adhere to APA style (though the journal in this case may have followed different guidelines).\n\n::: {.callout-note collapse=\"true\" icon=\"false\"} \n\n## Correlation Matrix - more than 1 comparison\n\nIf you want to correlate multiple variables with one another, you can do that in a correlation matrix.\n\nLet's try to replicate the correlations for full sample:\n\n* `OzEstimate` and `CalEstimate`, \n* `OzEstimate` and `M_postsoup`, and\n* `CalEstimate` and `M_postsoup`.\n\nTo do this, first ensure that only the relevant variables are selected and that a variable like `ParticipantID` is converted into a factor. I have stored all four variables in an object called `lopez_overall`.\n\nYou can also include an additional argument, `p_adjust`, to specify the type of correction you want to apply for multiple comparisons (e.g., \"Bonferroni\"). If you don’t include the `p_adjust` argument, the default method will be \"Holm\". In either case, the correction method will be listed in the output.\n\n```{r}\nlopez_overall <- lopez_reduced %>% \n  select(ParticipantID, OzEstimate, CalEstimate, M_postsoup)\n\ncorrelation(lopez_overall, p_adjust = \"bonferroni\")\n```\n::: {.callout-important} \n\nWhen running the code in the .Rmd file, the correction method is displayed beneath the table in the printout. However, when the document is knitted into HTML, this information does not appear.\n\nHowever, in the knitted document, the output also includes the number of observations for each row, whereas the .Rmd printout only displays the *range of observation* values beneath the table.\n\nSimilarly, p-values are displayed in APA style (e.g., *p* < .001) in the .Rmd printout, but in the knitted document, they may appear in scientific notation or as 0. **When reporting p-values in your report, ensure they adhere to APA style.**\n\n:::\n\n:::\n\n\n**The effect size for correlations does not require additional computation. It is simply the vale of the correlation coefficient `r`.**\n\n\n## Activity 9: Sensitivity power analysis\n\nAs with the t-test, we will conduct a sensitivity power analysis to determine the minimum effect size detectable with our sample of control participants (n = 244, accounting for the two missing values), an alpha level of 0.05, and a power of 0.8. This will help us assess whether our analysis was sufficiently powered.\n\nWe will use the `pwr.r.test()` function from the `pwr` package for the correlation analysis. As usual, include `n`, `alpha`, and `power` as arguments in the function, and leave out the effect size `r`.\n\n```{r}\npwr.r.test(n = 244, sig.level = 0.05, power = 0.8, alternative = \"two.sided\")\n```\nAccording to the output above, the smallest `r` detectable with 244 participants, a significance level of .05, and a power of .8 is .178. Since Lopez et al. reported an effect size of r = .41, the study was sufficiently powered.\n\n## Activity 10: The write-up\n\nLopez et al. (2024) hypothesised a positive correlation between estimated calories of soup consumed $(M = 133.0 cal, SD = 121.3 cal)$ and actual consumption volume $(M = 8.87 oz, SD = 6.24 oz)$. A Pearson correlation revealed a moderately strong, positive, and statistically significant relationship between these two variables that was sufficiently powered, $r(242) = .405, p < .001, 95\\% CI = [.294, .504]$. Lopez et al. rejected the null hypothesis in favour of H~1~.\n\n\n## Activity 11: Non-parametric alternative\n\nRunning a Pearson correlation when its assumptions are violated can lead to unreliable and misleading results, meaning the calculated correlation coefficient may not accurately reflect the true relationship between the variables.\n\nThe non-parametric alternative is **Spearman’s Rank correlation**. However, Spearman’s correlation also has a few assumptions:\n\n* Both variables must be measured on **ordinal, interval or ratio scales**.\n* There must be a **monotonic** relationship between the two variables, meaning they are either positively or negatively correlated. The data points should not display a \"U-shape\" or an \"inverted U-shape\".\n\nLet’s assess monotonicity with a scatterplot. Use the \"loess\" method to fit the curve, as we are not aiming to show a linear relationship this time.\n\n```{r}\nggplot(lopez_control_no_outliers, aes(x = CalEstimate, y = M_postsoup)) +\n  geom_point() +\n  geom_smooth(method = \"loess\")\n```\n**Verdict:** Both assumptions hold. The line of best fit displays a clear monotonic relationship, especially with the outliers removed. Therefore, we will proceed with Spearman’s correlation.\n\n\nBefore proceeding, we may want to recalculate the descriptives after removing the outliers:\n\n```{r}\n# descriptives for lopez_control_no_outliers\ndescriptives_control_no_outliers <- lopez_control_no_outliers %>% \n  summarise(n = n(),\n            mean_CalEstimate = mean(CalEstimate, na.rm = TRUE),\n            sd_CalEstimate = sd(CalEstimate, na.rm = TRUE),\n            mean_M_postsoup = mean(M_postsoup),\n            sd_M_postsoup = sd(M_postsoup)) %>% \n  ungroup()\n\ndescriptives_control_no_outliers\n```\n\n\nNext we will move on to the **inferential test**. The code remains the same as above, but this time we will change the method to \"Spearman\" and use the data object `lopez_control_no_outliers`.\n\n```{r}\ncorrelation(lopez_control_no_outliers, method = \"Spearman\")\n```\n\n\n\n**Writing-up the results** would be similar to the Pearson correlation above.\n\nIt was hypothesised that there would be a positive correlation between estimated calories of soup consumed $(M = 128.0 cal, SD = 105.4 cal)$ with actual consumption volume $(M = 8.68 oz, SD = 5.86 oz)$. The assumptions of linearity, normality, and homoscedasticity were assessed visually using a Residuals vs Fitted plot, a Q-Q plot, and a Scale-Location plot, respectively. These assumptions did not hold. Additionally, three outliers were removed from the dataset.\n\nConsequently, a Spearman’s rank correlation was conducted, revealing a strong, positive, and statistically significant relationship between estimated calories of soup consumed and actual consumption volume, $rho(239) = .522, p < .001, 95\\% CI = [.421, .611]$.\nThe study was sufficiently powered. Therefore, the null hypothesis is rejected in favour of H~1~.\n\n\n\n## [Test your knowledge]{style=\"color: #F39C12; text-transform: uppercase;\"} {.unnumbered}\n\n#### Question 1 {.unnumbered}\n\n**What is the main purpose of a Pearson correlation?**\n\n`r longmcq(sample(c(answer = \"To assess the strength and direction of a linear relationship between two continuous variables.\", x = \"To compare the means of two independent groups.\", x = \"To determine the monotonic relationship between two ordinal variables.\", x = \"To test for differences in variances between two continuous variables.\")))`\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n\n## Explain this answer\n\n**\"To assess the strength and direction of a linear relationship between two continuous variables.\"** is the correct answer.\n\n**The other options are incorrect:**\n\n* **\"To compare the means of two independent groups\"** is incorrect because that describes an independent t-test, not correlation.\n\n* **\"To determine the monotonic relationship between two ordinal variables\"** is incorrect because that describes Spearman’s correlation, not Pearson’s.\n\n* **\"To test for differences in variances between two continuous variables\"** is incorrect because that refers to Levene’s test or similar, not correlation.\n\n:::\n\n#### Question 2 {.unnumbered}\n\n**Which of the following is a key assumption of the Pearson correlation?**\n\n`r longmcq(sample(c(answer = \"The relationship between the two variables is linear.\", x = \"The two variables are measured on an ordinal, interval, or ratio scale\", x = \"The dependent variable is continuous.\", x = \"The test is robust against outliers.\")))`\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n\n## Explain this answer\n\nPearson correlation assumes linearity.\n\n**The other options are incorrect: **\n\n* **\"The two variables are measured on a ordinal scale\"** is incorrect because Pearson correlation requires interval or ratio data, but for ordinal data you would have to switch to the non-parametric equivalent (i.e. \"Spearman\").\n\n* **\"The dependent variable is continuous\"** is incorrect because there are no dependent and independent variables in a correlation. The variables are called \"measured variables\" or just \"variables\". And both of them need to be measured on a continuous scale.\n\n* **\"The test is robust against outliers\"** is incorrect because outliers can strongly influence the Pearson correlation coefficient, making it less robust in their presence.\n\n:::\n\n\n#### Question 3 {.unnumbered}\n\n**You perform a Pearson correlation and find $r(244) = .415, p = .002$. How would you interpret these results??**\n\n`r longmcq(sample(c(answer = \"There is a moderate, positive, statistically significant correlation between the two variables.\", x = \"There is a moderate, negative, statistically significant correlation between the two variables.\", x = \"There is a moderate, positive, statistically non-significant correlation between the two variables.\", x = \"There is a moderate, negative, statistically non-significant correlation between the two variables.\")))`\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n\n## Explain this answer\n\n**\"There is a moderate, positive, statistically significant correlation between the two variables.\"** is the correct answer. The correlation value ($r=.415$) indicates a moderate, positive relationship, and the p-value ($p=.002$) indicates statistical significance.\n\n:::\n\n\n\n#### Question 4 {.unnumbered}\n\n**What does a Spearman correlation coefficient of $rho = -.729$ indicate?**\n\n`r longmcq(sample(c(answer = \"A strong, negative monotonic relationship.\", x = \"A strong, positive monotonic relationship.\", x = \"A strong, negative linear relationship.\", x = \"A non-significant relationship.\")))`\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n\n## Explain this answer\n\n**\"A strong, negative monotonic relationship.\"** is the correct answer. The value of .729 indicates a strong relationship and the minus indicates it is negative. For Spearman the relationship has to be monotonic.\n\n**The other options are incorrect:**\n\n* **\"A strong, positive monotonic relationship\"** is incorrect because the minus implies the relationship is negative, not positive.\n\n* **\"A strong, negative linear relationship\"** is incorrect because Spearman measures monotonic relationships, not linear ones.\n\n* **\"A non-significant relationship\"** is incorrect because the question does not state the p-value or provide sample size information. Hence, we cannot assume anything about significance. For example, small samples with large rho (or r) values may not be statistically significant. Contrastingly, in large samples, smaller rho (or r) values can be significant.\n\n:::\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"kable","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["https://use.fontawesome.com/releases/v5.13.0/css/all.css","include/booktem.css","include/webex.css","include/glossary.css","include/style.css","include/custom.scss","include/styles.css"],"highlight-style":"a11y","include-after-body":["include/webex.js","include/script.js"],"output-file":"09-correlation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","lightbox":true,"bibliography":["include/references.bib"],"csl":"include/apa.csl","theme":{"light":["flatly","include/light.scss"],"dark":["darkly","include/dark.scss"]},"code-annotations":false,"anchor-sections":false,"code-copy":"hover","mainfont":"","monofont":""},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}