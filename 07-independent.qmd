# Two-sample t-test {#sec-independent}

```{r include=FALSE}
library(webexercises)
```

## Intended Learning Outcomes {.unnumbered}

In this chapter, we will focus on two-sample t-tests, also known as between-groups, between-subjects, or independent-samples t-tests. By the end of this chapter, you should be able to:

- Compute a two-sample t-test and effectively report the results.
- Understand when to use a non-parametric equivalent of the two-sample t-test, compute it, and report the results.


## [Individual Walkthrough]{style="color: #F39C12; text-transform: uppercase;"} {.unnumbered}

## Activity 1: Setup & download the data

This week, we will be working with a new dataset. Follow the steps below to set up your project:

* **Create a new project** and name it something meaningful (e.g., "2A_chapter7", or "07_independent_ttest"). See @sec-project if you need some guidance.
* **Create a new `.Rmd` file** and save it to your project folder. See @sec-rmd if you get stuck. 
* Delete everything after the setup code chunk (e.g., line 12 and below)
* **Download the reduced version of a new dataset** here: [data_ch7.zip](data/data_ch7.zip "download"). The zip folder includes the following files:
  * CodebookSimonTask.xlsx: A codebook with detailed variable descriptions
  * DemoSimonTask.csv: A CSV file containing demographic information
  * MeanSimonTask.csv: A CSV file with the mean response times
  * Sup_Mats_Simon_Task.docx: A Word document with Supplementary Materials providing additional details about the task
  * RawDataSimonTask.csv: The raw data file, allowing you to explore what experimental data looks like prior to pre-processing.
* Extract the data files from the zip folder and place them in your project folder. If you need help, see @sec-download_data_ch1.


**Citation**

> Zwaan, R. A., Pecher, D., Paolacci, G., Bouwmeester, S., Verkoeijen, P., Dijkstra, K., & Zeelenberg, R. (2018). Participant nonnaiveté and the reproducibility of cognitive psychology. *Psychonomic Bulletin & Review, 25*, 1968-1972. [https://doi.org/10.3758/s13423-017-1348-y](https://doi.org/10.3758/s13423-017-1348-y){target="_blank"}

The data and supplementary materials are available on OSF: [https://osf.io/ghv6m/](https://osf.io/ghv6m/){target="_blank"}

**Abstract**

> Many argue that there is a reproducibility crisis in psychology. We investigated nine well-known effects from the cognitive psychology literature—three each from the domains of perception/action, memory, and language, respectively—and found that they are highly reproducible. Not only can they be reproduced in online environments, but they also can be reproduced with nonnaïve participants with no reduction of effect size. Apparently, some cognitive tasks are so constraining that they encapsulate behavior from external influences, such as testing situation and prior recent experience with the experiment to yield highly robust effects.



**Changes made to the dataset**

* The dataset, demographic information, and Supplementary Materials have been reduced to include only information related to the Simon Task. The full dataset, which includes the other eight tasks, is available on OSF.
* No other changes were made.


## Activity 2: Library and data for today

Today, we’ll be using the following packages: `rstatix`, `tidyverse`, `car`, `lsr`, and `pwr`. You may need to install the packages before using them (see @sec-install_packages if you need some help). Make sure that `rstatix` is loaded in before tidyverse to avoid masking certain functions that we will need later.

We will also read in the data from `MeansSimonTask.csv`  and the demographic information from `DemoSimonTask.csv`.

```{r eval=FALSE}
# load in the packages
???

# read in the data
zwaan_data <- ???
zwaan_demo <- ???
```


```{r include=FALSE, message=TRUE}
## I basically have to have 2 code chunks since I tell them to put the data files next to the project, and mine are in a separate folder called data - unless I'll turn this into a fixed path
library(rstatix)
library(tidyverse)
library(car)
library(lsr)
library(pwr)

zwaan_data <- read_csv("data/MeansSimonTask.csv")
zwaan_demo <- read_csv("data/DemoSimonTask.csv")
```




::: {.callout-caution collapse="true" icon="false"} 

## Solution 

```{r eval=FALSE}
# load in the packages
library(rstatix)
library(tidyverse)
library(car)
library(lsr)
library(pwr)

# read in the data
zwaan_data <- read_csv("MeansSimonTask.csv")
zwaan_demo <- read_csv("DemoSimonTask.csv")
```

:::

## Activity 3: Familiarise yourself with the data

As usual, take some time to familiarise yourself with the data before starting on the between-subjects t-test. Also, more importantly, have a look at the Supplementary Materials in which the Simon effect is explained in more depth.

In general, **the Simon effect** refers to the phenomenon where participants respond faster when the stimulus appears on the same side of the screen as the button they need to press (i.e., a congruent condition). Conversely, response times are slower when the stimulus appears on the opposite side of the screen from the required button (i.e., an incongruent condition).

In this experiment, all participants completed two sessions of trials. However, they were divided into two groups based on the stimuli they received:

* **Same Stimuli Group**: Half of the participants received the same set of stimuli in both sessions.
* **Different Stimuli Group**: The other half received a different set of stimuli in session 2 compared to session 1.

### Potential research questions and hypotheses

* **Potential research question**: “Is there a significant difference in the Simon effect between participants who received the same stimuli in both sessions compared to those who received different stimuli?”
* **Null Hypothesis (H~0~)**: "There is no significant difference in the Simon effect between participants who received the same stimuli in both sessions and those who received different stimuli."
* **Alternative Hypothesis (H~1~)**: "There is a significant difference in the Simon effect between participants who received the same stimuli in both sessions and those who received different stimuli."



## Activity 4: Preparing the dataframe

The data is already in a very good shape, however, we need to perform some data wrangling to compute the Simon effect.


**Steps to calculate the Simon effect:**

1. For each participant, **compute the mean response time (RT)** for congruent trials and the mean RT for incongruent trials
2. Subtract the mean RT of congruent trials from the mean RT of incongruent trials to **calculate the Simon effect**

To streamline analysis, we should join this output with the demographics data to have all relevant information in one place.

Basically, we want to create a tibble that has the following content. *[Note that I re-arranged the columns and re-labelled some of them in a final step, so your column names and/or order might be slightly different, but content should match.]*

```{r echo=FALSE, warning=FALSE, message=FALSE}
simon_effect <- zwaan_data %>% 
  pivot_longer(cols = session1_congruent:session2_incongruent, names_to = "col_headings", values_to = "RT") %>% 
  separate(col_headings, into = c("Session_number", "congruency"), sep = "_") %>% 
  group_by(participant, similarity, congruency) %>% 
  summarise(mean_RT = mean(RT)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = congruency, values_from = mean_RT) %>% 
  mutate(simon_effect = incongruent - congruent) %>% 
  full_join(zwaan_demo, by = join_by(participant == twosubjectnumber)) %>% 
  select(participant, gender = gender_response, age = age_response, education = education_response, similarity:simon_effect)

head(simon_effect, n = 5)
```

Obviously, there are various ways to achieve this, so feel free to explore and come up with your own approach. However, we will provide step-by-step instructions for one of those ways that will get you the desired output.

::: {.callout-note collapse="true" icon="false"}

## Hints

* **Step 1**: Convert the data from wide format to long format, so that all RT values are consolidated into a single column. This transformation will result in each participant having four rows.
* **Step 2**: There should be a column now that contained the previous column headings with information on session number and congruency. Separate this information into 2 separate columns (i.e., session number and congruency).
* **Step 3**: Compute the mean RT values for each combination of `participant`, `similarity`, and `congruency`
* **Step 4**: Pivot the data back to wide format so that the mean RT values for congruent and incongruent trials are placed in two separate columns.
* **Step 5**: Add a new column called the `simon_effect` that calculates the Simon effect by subtracting the mean RT for congruent trials from the mean RT for incongruent trials.
* **Step 6**: Merge this processed dataset with the demographics data to ensure all relevant information is in one table.
* **Step 7**: Feel free to rearrange the order of columns and/or rename them to match your output with ours (not strictly necessary tbh)

::: {.callout-caution collapse="true" icon="false"}

## Solution to the steps outlined above

```{r eval=FALSE}
simon_effect <- zwaan_data %>% 
  pivot_longer(cols = session1_congruent:session2_incongruent, names_to = "col_headings", values_to = "RT") %>% 
  separate(col_headings, into = c("Session_number", "congruency"), sep = "_") %>% 
  group_by(participant, similarity, congruency) %>% 
  summarise(mean_RT = mean(RT)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = congruency, values_from = mean_RT) %>% 
  mutate(simon_effect = incongruent - congruent) %>% 
  full_join(zwaan_demo, by = join_by(participant == twosubjectnumber)) %>% 
  select(participant, gender = gender_response, age = age_response, education = education_response, similarity:simon_effect)
```

:::

:::


## Activity 5: Compute descriptives

Next, we want to compute number of participants (**n**), **means** and **standard deviations** for each group (i.e., `same` and `different`) of our variable of interest (i.e., `simon_effect`).


::: {.callout-caution collapse="true" icon="false"}

## Solution

```{r}
descriptives <- simon_effect %>% 
  group_by(similarity) %>% 
  summarise(n = n(),
            mean_RT = mean(simon_effect),
            sd_RT = sd(simon_effect)) %>% 
  ungroup()

descriptives
```

:::


## Activity 6: Create an appropriate plot

Which plot would you choose to represent the data appropriately? Create a plot that effectively visualises the data, and then compare it with the solution provided below.

::: {.callout-caution collapse="true" icon="false"} 

## Solution 

```{r}
ggplot(simon_effect, aes(x = similarity, y = simon_effect, fill = similarity)) +
  geom_violin(alpha = 0.5) +
  geom_boxplot(width = 0.4, alpha = 0.8) +
  scale_fill_viridis_d(guide = "none") +
  theme_classic() +
  labs(x = "Similarity", y = "Simon effect")
```

:::

## Activity 7: Check assumptions

#### Assumption 1: Continuous DV {.unnumbered}

The dependent variable must be measured at interval or ratio level. We can confirm that by looking at `simon_effect`. 


#### Assumption 2: Data are independent {.unnumbered}

There should be no relationship between the observations. Scores in one condition or observation should not influence scores in another. We assume this assumption holds for our data.


#### Assumption 3: Homoscedasticity (homogeneity of variance) {.unnumbered}

This assumption requires the variances between the two groups to be similar (i.e., homoscedasticity). If the variances between the 2 groups are dissimilar/unequal, we have heteroscedasticity.

We can test this using a **Levene’s Test for Equality of Variance** which is available in the package `car`. The first argument specifies the formula in the format `DV ~ IV`. Here:

* The dependent variable (DV) is `simon_effect` (continuous)
* The independent variable (IV) is `similarity` (the grouping variable)

To perform the test, separate the variables with a tilde (`~`), and specify the dataset using the `data` argument:

```{r}
leveneTest(simon_effect ~ similarity, data = simon_effect)
```

The warning message tells us that the grouping variable was converted into a factor. Oops, I guess we forgot to convert `similarity` into a factor during data wrangling.

The test output shows a **p-value greater than .05**. This indicates that we do not have enough evidence to reject the null hypothesis. Therefore, the variances across the two groups can be assumed equal.

You would report this result in APA style: A Levene's test of homogeneity of variances was used to compare the variances of the same and the different groups. The test indicated that the variances were homogeneous, $F(1,158) = 0.73, p = .395$.

::: {.callout-important}

The t-test we are conducting is a Welch t-test by default. The Welch t-test provides similar results to a Student’s t-test when variances are equal but is preferred when variances are unequal.

This means that even if Levene’s test returns a significant p-value, indicating that the variances between the groups are unequal, the Welch t-test remains appropriate and valid for analysis.

:::



#### Assumption 4: DV should be approximately normally distributed {.unnumbered}

It’s important to note that this assumption requires the **dependent variable to be normally distributed within each group**.

We can either use our eyeballs again on the violin-boxplot we created earlier (or use a qqplot, density plot, or histogram instead), OR compute a statistic like the Shapiro-Wilk's test we already mentioned previously for the one-sample t-test. However, keep in mind that with large sample sizes (approximately 80 participants per group), this test may flag minor deviations from normality as significant, even if the data is reasonably normal.


Visual inspection suggests that both groups are approximately normally distributed. The **"same" group** appears slightly more normally distributed than the **"different" group**, which has a small peak in the lower tail. Despite this, both distributions seem normal enough for practical purposes with real-world data.

::: {.callout-tip} 

## Tip: Visual inspection

If you want to use a histogram, density plot or qqplot (the ones created with the `ggplot2` and `qqplotr` packages), you can simply add a `facet_wrap()` function to display the plots separately for each group. 

If you are using the Q-Q plot function from the `car` package, you will need to create separate data objects filtered for each group before generating Q-Q plots for the groups individually.

:::

::: {.callout-note icon="false"} 

## Task: Compute Shapiro-Wilk's test

As mentioned above, the function for the **Shapiro-Wilk's test** does not allow for a formula. This means you will need to create separate data objects for the two groups first. Consider this a good opportunity to practice using the `filter()` function.

**Step-by-step instructions**: 

1. Create separate data objects for the same and different groups.
2. Run the Shapiro-Wilk test on each group’s data.
3. Examine the results. What do you conclude from the test outcomes?

::: {.callout-caution collapse="true" icon="false"} 

## Solution 

```{r}
## same group
same <- simon_effect %>% 
  filter(similarity == "same")

shapiro.test(same$simon_effect)

## different group
different <- simon_effect %>% 
  filter(similarity == "different")

shapiro.test(different$simon_effect)
```

**Interpretation:**

Shapiro-Wilk's test also suggests that the data for both groups, "same" and "different", are normally distributed as all p-values are above .05. 

Again, if you used this method in your report, you would have to write up the results in APA style (refer to the section on the one-sample t-test for guidance on reporting).

:::

:::


::: {.callout-important}

If you have read the Delacre et al. (2017) paper ([https://rips-irsp.com/articles/10.5334/irsp.82](https://rips-irsp.com/articles/10.5334/irsp.82){target="_blank"}), you might be aware that the **normality assumption is not critical for the Welch t-test**.

This means that, whether you consider both groups to be "normally distributed" or interpret one as slightly deviating from normality, the Welch t-test remains an appropriate choice for this dataset.

:::

After verifying all the assumptions, we concluded that they were met. Therefore, we will compute a **Welch two-sample t-test**.





## Activity 8: Compute a Two-sample t-test and effect size

The `t.test()` function, which we previously used for the one-sample t-test, can also be used here, but with a slightly different approach. It supports a formula option, which simplifies the process. This means we don’t need to wrangle the data further or use the `$` operator to access columns directly. Instead, we can specify the formula as `DV ~ IV`.

The key arguments for `t.test()` are:

* The first argument in the formula with the pattern `DV ~ IV`
* The second argument is the data
* The third argument is specifying whether variances are equal between the groups. The default value is `var.equal = FALSE`, which conducts a **Welch t-test**. If you set `var.equal = TRUE`, you would conduct a Student t-test instead.
* The 4th argument `alternative` specifies the alternative hypothesis. The default value is "two.sided", meaning the test will check for differences in both directions (i.e., a non-directional hypothesis)


```{r}
t.test(simon_effect ~ similarity, data = simon_effect, var.equal = FALSE, alternative = "two.sided")
```

The output of the `t.test()` function tells us:

* the **type of test** that was conducted (here Welch t-test)
* the **variables** that were tested (here `simon_effect` by `similarity`), 
* the **t-value**, **degrees of freedom**, and **p**,
* the **alternative hypothesis** tested, 
* a **95% confidence interval** for the difference between group means, and
* the **mean of both groups** (which should match our descriptive stats)


The `t.test()` function does not calculate an **effect size**, so we have to compute it separately once again. As with the one-sample t-test, we can use the `CohensD()` function from the `lsr` package. The formula-based approach works here too. For the **Welch version** of the t-test, you need to include the argument `method = "unequal"` in the `CohensD()` function to account for unequal variances.


```{r}
cohensD(simon_effect ~ similarity, data = simon_effect, method = "unequal")
```


## Activity 9: Sensitivity power analysis

Next, we will conduct a **sensitivity power analysis** to determine the minimum effect size that could have been reliably detected with our sample size, an alpha level of 0.05, and a power of 0.8.

To perform this analysis, we use the `pwr.t.test()` from the `pwr` package. The arguments are the same as those used for the one-sample t-test, but with a few adjustments:

* **Number of Participants**: Specify the number of observations per sample (i.e., n)
* **Test Type**: Set the `type` argument to "two.sample" for a two-sample t-test.


```{r}
pwr.t.test(n = 80, sig.level = 0.05, power = 0.8, type = "two.sample", alternative = "two.sided")
```

Sooo, the smallest effect size we can detect with a sample size of 80 participants in each group, an alpha level of .05, and power of .8 is **0.45**. This is larger than the actual effect size we calculated using the `CohensD()` function (i.e., 0.14). This indicates that our analysis is underpowered to detect such a small effect size.

#### Hypothetical Replication Study {.unnumbered}

Out of curiosity, if we were to replicate this study and wanted to reliably detect an effect size as small as **0.14**, how many participants would we need?

We can use the `pwr.t.test()` function again. This time, instead of specifying `n`, we provide the effect size (`d = 0.14`). The result shows that we would need approximately **1,500 participants in total** (750 per group). Ooft; that's quite a few people to recruit.

However, it’s worth noting that an effect size of 0.14 may not be practically meaningful, even if statistically detectable.

```{r}
pwr.t.test(d = 0.145, sig.level = 0.05, power = 0.8, type = "two.sample", alternative = "two.sided")
```


::: {.callout-tip}

## But my two groups have unequal sample sizes, and there is only one n in `pwr.t.test`. What do I do?

No problem! You can use the `pwr.t2n.test()` function, which allows you to specify different sample sizes for the two groups (`n1` and `n2`). 

The rest of the arguments are essentially the same as with `pwr.t.test()`. Additionally, there is no need to specify the `type` argument, as the function is specifically designed for two-sample t-tests with unequal sample sizes.

```{r eval=FALSE}
pwr.t2n.test(n1 = NULL, n2= NULL, d = NULL, sig.level = 0.05, power = NULL, alternative = c("two.sided", "less","greater"))
```

Let's try it for our example. We should get the same result though.

```{r}
pwr.t2n.test(n1 = 80, n2= 80, sig.level = 0.05, power = 0.8, alternative = "two.sided")
```

:::


## Activity 10: The write-up

We hypothesised that there would be a significant difference in the Simon effect between participants who received the same stimuli in both sessions $(N = 80, M = 35.99 \text{msec}, SD = 22.40 \text{msec})$ and those who received different stimuli $(N = 80, M = 32.86 \text{msec}, SD = 20.79 \text{msec})$. Using a Welch two-sample  t-test, the effect was found to be non-significant and of a small magnitude, $t(157.14) = 0.92, p = .360, d = 0.15$. The overall mean difference between groups was small $(M_{diff} = 3.14 \text{msec})$. Therefore, we fail to reject the null hypothesis.



## Activity 11: Non-parametric alternative {#sec-alternative_two_sample}

The **Mann-Whitney U-test** is the non-parametric equivalent of the independent two-sample t-test. It is used to compare the medians of two samples and is particularly useful when the assumptions of the t-test are not met.

According to Delacre et al. (2017), the Mann-Whitney U-test is robust to violations of normality but remains sensitive to heteroscedasticity. In this case, we don’t need to worry about heteroscedasticity, as the variances in the two groups are equal. However, it’s important to keep this in mind when assessing assumptions and interpreting results with other datasets.


First, let's start by computing some **summary statistics** for each group.

```{r}
simon_effect %>% 
  group_by(similarity) %>% 
  summarise(n = n(), 
            median = median(simon_effect))
```


To **conduct a Mann-Whitney U-test**, use the `wilcox.test()` function. As with the independent t-test, you can use the formula approach `DV ~ IV`. The code structure is identical to what we used for the independent t-test.

```{r}
wilcox.test(simon_effect ~ similarity, data = simon_effect)
```


We should compute the **standardised test statistic Z** manually. To do this, use the `qnorm()` function on the halved p-value obtained from the Wilcoxon test conducted earlier.

```{r}
# storing the p-value
p_wilcoxon <- wilcox.test(simon_effect ~ similarity, data = simon_effect)$p.value

# calculate the z value from half the p-value
z = qnorm(p_wilcoxon/2)
z

```


The **effect size** for the Mann-Whitney U-test is **r**. To compute r, we'd need the standardised test statistic z and divide that the square-root of the number of pairs n: $r = \frac{|z|}{\sqrt n}$. 

Alternatively, you can use the `wilcox_effsize()` function from the `rstatix` package to simplify the process.

The arguments for this function are slightly different in order but otherwise identical to those used in the `wilcox.test()` function above.

```{r}
wilcox_effsize(data = simon_effect, formula = simon_effect ~ similarity)
```

This is once again considered a small effect. Anyway, we do have all the numbers now to **write up the results**:


A Mann-Whitney U-test was conducted to determine whether there was a significant difference in the Simon effect between participants who received the same stimuli in both sessions $(N = 80, Mdn = 35.68 \text{msec})$ and those who received different stimuli $(N = 80, Mdn = 34.44 \text{msec})$. The results indicate that the median difference in response time was non-significant and of small magnitude, $W = 3001, Z = -0.68, p = .498, r = .054$. Therefore, we fail to reject the null hypothesis.



## [Test your knowledge]{style="color: #F39C12; text-transform: uppercase;"} {.unnumbered}

#### Question 1 {.unnumbered}

**What is the main purpose of an independent t-test?**

`r longmcq(sample(c(answer = "To compare means between two independent groups", x = "To compare means between two related groups", x = "To assess the correlation between two continuous variables", x = "To test for differences in variances between two independent groups")))`

::: {.callout-caution collapse="true" icon="false"}

## Explain this answer

The independent t-test is specifically designed to compare the means of two separate (independent) groups to determine whether the difference between their means is statistically significant. For example, it could be used to compare the test scores of students who received two different teaching methods (Group 1 vs. Group 2).

:::

#### Question 2 {.unnumbered}

**Which of the following is a key assumption of the two-sample t-test that should be considered?**

`r longmcq(sample(c(answer = "The observations are independent of each other", x = "The independent variable is normally distributed", x = "The dependent variable is categorical", x = "The sample sizes must be equal")))`

::: {.callout-caution collapse="true" icon="false"}

## Explain this answer

Independence of observations is a crucial assumption for the two-sample t-test. It means that the data collected from one participant should not influence the data from another participant.

:::


#### Question 3 {.unnumbered}

**How can you recognise the difference between the output of a Student’s t-test and a Welch t-test?**

`r longmcq(sample(c(answer = "The Welch t-test reports non-integer degrees of freedom, while the Student’s t-test reports integer degrees of freedom.", x = "The Welch t-test includes a confidence interval, while the Student’s t-test does not.", x = "The Welch t-test uses medians instead of means, unlike the Student’s t-test.", x = "The Welch t-test reports an effect size directly, while the Student’s t-test does not.")))`

::: {.callout-caution collapse="true" icon="false"}

## Explain this answer

The Welch t-test adjusts for unequal variances and sample sizes, which leads to non-integer degrees of freedom. In contrast, the Student’s t-test assumes equal variances and reports integer degrees of freedom based on total sample size minus the number of groups.

The other options are incorrect: 

* Both tests report confidence intervals for the mean difference.
* Both tests compare means, not medians.
* Neither test reports effect size directly; it must be calculated separately (e.g., using Cohen’s d).

:::



#### Question 4 {.unnumbered}

**You perform an independent t-test and find $t(48)=2.10,p=.042,d=0.58$. How would you interpret these results?**

`r longmcq(sample(c(x = "There is no significant difference between the two groups", x = "There is a significant difference between the two groups, with a large effect size.", answer = "There is a significant difference between the two groups, with a medium effect size.", x = "There is a significant difference between the two groups, with a small effect size.")))`

::: {.callout-caution collapse="true" icon="false"}

## Explain this answer

The p-value ($p=.042$) is less than the common significance threshold of 0.05, indicating that the difference between the two groups is statistically significant. This means we reject the null hypothesis and conclude that there is evidence of a difference between the group means.

:::


